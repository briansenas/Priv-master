{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CLASIFICACIÓN DEL RIESGO DE ABANDONO DE LOS CLIENTES DE UN BANCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos con el que vamos a trabajar ahora contiene información sobre los usuarios de un banco. Queremos predecir si los clientes van a dejar de usar los servicios de dicho banco o no. El conjunto de datos consta de 10000 observaciones y 14 variables.\n",
    "\n",
    "La siguiente figura indica cómo cargar el conjunto de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una matriz con las variables de entrada y otra matriz con la variable de salida (objetivo, columna 14). Excluiremos la columna 1 y 2 que son ‘row_number’ y ‘customerid’ ya que no nos aportan información útil para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,3:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 'France', 'Female', 39, 1, 0.0, 2, 0, 0, 93826.63]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer el análisis más sencillo si codificamos las variables no numéricas. Country contiene los valores: ’France, Spain, Germany’ y Gender: ‘Male, Female’. La manera de codificarlo será convertir estas palabras a valores numéricos. Para esto usaremos la función LabelEncoder, de la librería ‘ScikitLearn’, que al darle una cadena de texto nos devuelve valores entre 0 y n_clases-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [608, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [502, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [772, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que Country ahora toma valores del 0 al 2 mientras que male y female fueron reemplazados por 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función train_test_split de la librería ScikitLearn para dividir nuestros datos.\n",
    "\n",
    "Usaremos 80% para entrenar el modelo y 20% para validarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[686, 0, 1, ..., 1, 1, 179093.26],\n",
       "        [632, 1, 1, ..., 1, 1, 195978.86],\n",
       "        [559, 2, 1, ..., 1, 0, 85891.02],\n",
       "        ...,\n",
       "        [735, 0, 0, ..., 0, 0, 92220.12],\n",
       "        [667, 0, 1, ..., 1, 0, 97508.04],\n",
       "        [697, 1, 1, ..., 1, 1, 53581.14]], dtype=object),\n",
       " array([[596, 1, 1, ..., 0, 0, 41788.37],\n",
       "        [623, 0, 1, ..., 1, 1, 146379.3],\n",
       "        [601, 2, 0, ..., 1, 0, 58561.31],\n",
       "        ...,\n",
       "        [730, 0, 0, ..., 1, 0, 33373.26],\n",
       "        [692, 0, 1, ..., 1, 0, 76755.99],\n",
       "        [628, 1, 1, ..., 1, 1, 107674.3]], dtype=object),\n",
       " array([0, 0, 1, ..., 1, 1, 0]),\n",
       " array([0, 0, 0, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observamos los datos detenidamente podemos apreciar que hay variables cuyos valores pueden\n",
    "ser muy variados, desde muy altos a muy pequeños por esta razón escalaremos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.35649971, -0.9055496 ,  0.91324755, ...,  0.64920267,\n",
       "          0.97481699,  1.36766974],\n",
       "        [-0.20389777,  0.30164867,  0.91324755, ...,  0.64920267,\n",
       "          0.97481699,  1.6612541 ],\n",
       "        [-0.96147213,  1.50884694,  0.91324755, ...,  0.64920267,\n",
       "         -1.02583358, -0.25280688],\n",
       "        ...,\n",
       "        [ 0.86500853, -0.9055496 , -1.09499335, ..., -1.54035103,\n",
       "         -1.02583358, -0.1427649 ],\n",
       "        [ 0.15932282, -0.9055496 ,  0.91324755, ...,  0.64920267,\n",
       "         -1.02583358, -0.05082558],\n",
       "        [ 0.47065475,  0.30164867,  0.91324755, ...,  0.64920267,\n",
       "          0.97481699, -0.81456811]]),\n",
       " array([[-0.57749609,  0.30164867,  0.91324755, ..., -1.54035103,\n",
       "         -1.02583358, -1.01960511],\n",
       "        [-0.29729735, -0.9055496 ,  0.91324755, ...,  0.64920267,\n",
       "          0.97481699,  0.79888291],\n",
       "        [-0.52560743,  1.50884694, -1.09499335, ...,  0.64920267,\n",
       "         -1.02583358, -0.72797953],\n",
       "        ...,\n",
       "        [ 0.81311987, -0.9055496 , -1.09499335, ...,  0.64920267,\n",
       "         -1.02583358, -1.16591585],\n",
       "        [ 0.41876609, -0.9055496 ,  0.91324755, ...,  0.64920267,\n",
       "         -1.02583358, -0.41163463],\n",
       "        [-0.24540869,  0.30164867,  0.91324755, ...,  0.64920267,\n",
       "          0.97481699,  0.12593183]]),\n",
       " array([0, 0, 1, ..., 1, 1, 0]),\n",
       " array([0, 0, 0, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez escalados los datos, pasamos a construir la red neuronal. Importamos Keras, usamos el módulo Sequential para inicializar la red y el modelo Dense para añadir capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la red con Sequential()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las capas usando la función Dense. Indicamos el número de nodos que queremos añadir con output_dim, Init es la inicialización del descenso de gradiente estocástico. Los pesos iniciales serán una variable aleatoria uniforme. Input_dim sólo es necesaria en la primera capa para que el modelo sepa la cantidad de variables que va a recibir, en nuestro caso 11. A partir de aquí las siguientes capas heredarán esta cualidad de la primera capa. La función de activación que utilizaremos será relu en las dos primeras capas (cuanto más cerca tenga su valor a 1, la neurona estará más activada y tendrá más interacción) y en la capa final hemos utilizado la función sigmoide ya que nuestro objetivo es clasificar.\n",
    "\n",
    "Una vez que tenemos la configuración específica de la red, la siguiente tarea es compilarla, para eso utilizamos la función Compile. El primer argumento de esta función es Optimizer que indica el método para entrenar los pesos. Adam es un algoritmo que se basa en el cálculo del descenso del Gradiente Estocástico. El segundo parámetro es loss, este usará la función ‘binary_crossentropy’ para clasificar en 2 categorías. Si tuviéramos más categorías utilizaríamos la función ‘categorical_crossentropy’. Para saber la bondad de nuestra red neuronal utilizaremos la métrica accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briansenas/Priv-master/SoftComputing/Optimizacion/guiones/.venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(6, activation = 'relu', input_shape = (10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función fit para ajustar los pesos de la red. Batch_size para especificar el número de observaciones que necesita entrenar antes de actualizar los pesos. Epoch nos indica el número de iteraciones que realizaremos en el entrenamiento. La estimación de estos parámetros se tiene que hacer por ensayo-error, probando con diferentes valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 803us/step - accuracy: 0.7842 - loss: 0.5117\n",
      "Epoch 2/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 823us/step - accuracy: 0.7862 - loss: 0.4542\n",
      "Epoch 3/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 797us/step - accuracy: 0.8240 - loss: 0.4163\n",
      "Epoch 4/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 778us/step - accuracy: 0.8400 - loss: 0.3840\n",
      "Epoch 5/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 792us/step - accuracy: 0.8461 - loss: 0.3750\n",
      "Epoch 6/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 821us/step - accuracy: 0.8452 - loss: 0.3704\n",
      "Epoch 7/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 811us/step - accuracy: 0.8482 - loss: 0.3656\n",
      "Epoch 8/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 812us/step - accuracy: 0.8517 - loss: 0.3619\n",
      "Epoch 9/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 825us/step - accuracy: 0.8504 - loss: 0.3600\n",
      "Epoch 10/10\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 801us/step - accuracy: 0.8481 - loss: 0.3587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x782360f95bd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la predicción sobre nuestro conjunto de test lo haremos mediante la siguiente expresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción nos proporcionará la probabilidad de pertenecer a un grupo u otro, de tal manera que aquellos valores mayores que 0.5 serán 1 y el resto 0.\n",
    "\n",
    "Creamos una matriz de confusión y vemos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1562,   45],\n",
       "       [ 234,  159]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92      1607\n",
      "           1       0.78      0.40      0.53       393\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.82      0.69      0.73      2000\n",
      "weighted avg       0.85      0.86      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: Búsqueda del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:,3:13].values\n",
    "y = dataset.iloc[:,13].values\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.6.1\n",
      "Uninstalling scikit-learn-1.6.1:\n",
      "  Successfully uninstalled scikit-learn-1.6.1\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/briansenas/Priv-master/SoftComputing/Optimizacion/guiones/.venv/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/briansenas/Priv-master/SoftComputing/Optimizacion/guiones/.venv/lib/python3.10/site-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/briansenas/Priv-master/SoftComputing/Optimizacion/guiones/.venv/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/briansenas/Priv-master/SoftComputing/Optimizacion/guiones/.venv/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.15.1)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2\n"
     ]
    }
   ],
   "source": [
    "# To be able to import the sklear-wrapper\n",
    "# !pip install scikeras\n",
    "# !pip uninstall -y scikit-learn\n",
    "# !pip install scikit-learn==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'class_weight'])\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 804us/step - loss: 0.5092\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 776us/step - loss: 0.4322\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 781us/step - loss: 0.4196\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 766us/step - loss: 0.4098\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 763us/step - loss: 0.3831\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 767us/step - loss: 0.3706\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 775us/step - loss: 0.3657\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 770us/step - loss: 0.3622\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 780us/step - loss: 0.5218\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 766us/step - loss: 0.4376\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 767us/step - loss: 0.4210\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 763us/step - loss: 0.4138\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 769us/step - loss: 0.4070\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 772us/step - loss: 0.3992\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 766us/step - loss: 0.3858\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 772us/step - loss: 0.3732\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 776us/step - loss: 0.5083\n",
      "Epoch 2/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 765us/step - loss: 0.4189\n",
      "Epoch 3/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 762us/step - loss: 0.3841\n",
      "Epoch 4/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 764us/step - loss: 0.3752\n",
      "Epoch 5/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 765us/step - loss: 0.3702\n",
      "Epoch 6/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 761us/step - loss: 0.3605\n",
      "Epoch 7/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 757us/step - loss: 0.3624\n",
      "Epoch 8/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 760us/step - loss: 0.3571\n",
      "\u001b[1m2666/2666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 826us/step - loss: 0.5054\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.4026\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 794us/step - loss: 0.3710\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 809us/step - loss: 0.3626\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 811us/step - loss: 0.3653\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 806us/step - loss: 0.3578\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 804us/step - loss: 0.3617\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 796us/step - loss: 0.3562\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 795us/step - loss: 0.5082\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 850us/step - loss: 0.4460\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 846us/step - loss: 0.4347\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 835us/step - loss: 0.4069\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 819us/step - loss: 0.3920\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 794us/step - loss: 0.3798\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 792us/step - loss: 0.3706\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 798us/step - loss: 0.3610\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 496us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 821us/step - loss: 0.5400\n",
      "Epoch 2/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 808us/step - loss: 0.4153\n",
      "Epoch 3/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 834us/step - loss: 0.3802\n",
      "Epoch 4/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 804us/step - loss: 0.3655\n",
      "Epoch 5/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 794us/step - loss: 0.3561\n",
      "Epoch 6/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 794us/step - loss: 0.3516\n",
      "Epoch 7/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.3464\n",
      "Epoch 8/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.3532\n",
      "\u001b[1m2666/2666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 793us/step - loss: 0.4792\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783us/step - loss: 0.4161\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 801us/step - loss: 0.3932\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 809us/step - loss: 0.3750\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783us/step - loss: 0.3643\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 775us/step - loss: 0.3567\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 782us/step - loss: 0.3568\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 769us/step - loss: 0.3495\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 820us/step - loss: 0.5053\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 803us/step - loss: 0.4373\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 808us/step - loss: 0.4177\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 769us/step - loss: 0.4000\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 742us/step - loss: 0.3806\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 749us/step - loss: 0.3718\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 743us/step - loss: 0.3669\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 750us/step - loss: 0.3616\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 766us/step - loss: 0.5032\n",
      "Epoch 2/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 773us/step - loss: 0.4137\n",
      "Epoch 3/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 782us/step - loss: 0.3936\n",
      "Epoch 4/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 737us/step - loss: 0.3851\n",
      "Epoch 5/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 750us/step - loss: 0.3753\n",
      "Epoch 6/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 772us/step - loss: 0.3691\n",
      "Epoch 7/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 744us/step - loss: 0.3655\n",
      "Epoch 8/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 742us/step - loss: 0.3672\n",
      "\u001b[1m2666/2666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 847us/step - loss: 0.4805\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 759us/step - loss: 0.4099\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 762us/step - loss: 0.3767\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 761us/step - loss: 0.3745\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 761us/step - loss: 0.3651\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 762us/step - loss: 0.3659\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 766us/step - loss: 0.3649\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 765us/step - loss: 0.3591\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 756us/step - loss: 0.5422\n",
      "Epoch 2/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 759us/step - loss: 0.4184\n",
      "Epoch 3/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 756us/step - loss: 0.4018\n",
      "Epoch 4/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 758us/step - loss: 0.3922\n",
      "Epoch 5/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 748us/step - loss: 0.3818\n",
      "Epoch 6/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 756us/step - loss: 0.3808\n",
      "Epoch 7/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 754us/step - loss: 0.3728\n",
      "Epoch 8/8\n",
      "\u001b[1m5333/5333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 757us/step - loss: 0.3697\n",
      "\u001b[1m2667/2667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 768us/step - loss: 0.5122\n",
      "Epoch 2/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 752us/step - loss: 0.4297\n",
      "Epoch 3/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 756us/step - loss: 0.4128\n",
      "Epoch 4/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 751us/step - loss: 0.4009\n",
      "Epoch 5/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 754us/step - loss: 0.3873\n",
      "Epoch 6/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 745us/step - loss: 0.3821\n",
      "Epoch 7/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 751us/step - loss: 0.3743\n",
      "Epoch 8/8\n",
      "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 743us/step - loss: 0.3609\n",
      "\u001b[1m2666/2666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.5050\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.4594\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.4458\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.4353\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.4268\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 0.4200\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.4192\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.4181\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.5756 \n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.4808\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.4550\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 0.4341\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.4210\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.4155\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 0.3992\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 0.3904\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.5780\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.4672\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.4351\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.4240\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.4122\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.3986\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.3885\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 0.3811\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6977\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 0.5008\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.4606\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.4400\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.4266\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.4256\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.4229\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.4108\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.6066\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.4841\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.4694\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.4603\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.4448\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.4438\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.4461\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.4381\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6063\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.4890\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.4672\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 0.4556\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.4434\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.4406\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 0.4319\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.4259\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.6377\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.4418\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.4273\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.4179\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 0.4098\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.4066\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.3979\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.3958\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.6138\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.4501\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.4457\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.4415\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.4399\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 0.4379\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.4300\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.4251\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.6095\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.4508\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 0.4374\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.4301\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.4207\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.4141\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.4005\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.3948\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.5834\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.4490\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.4325\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.4264\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.4192\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.4112\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.3979\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 0.3929\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.6091\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.4548\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.4400\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.4280\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 0.4252\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.4180\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.4148\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.4048\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6265\n",
      "Epoch 2/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.4574\n",
      "Epoch 3/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 0.4444\n",
      "Epoch 4/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 0.4366\n",
      "Epoch 5/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.4282\n",
      "Epoch 6/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.4150\n",
      "Epoch 7/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.4106\n",
      "Epoch 8/8\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.4024\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6735\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.4877\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 0.4600\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.4460\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.4429\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.4312\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.4227\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.4137\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6202\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.4910\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.4665\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 0.4543\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.4463\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 0.4411\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.4389\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 0.4337\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6056\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.4993\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.4759\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.4598\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.4484\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 0.4382\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.4353\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.4340\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.6759\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 0.5025\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.4658\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 0.4552\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.4409\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.4367\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.4307\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.4301\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.5900\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.5045\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 0.4866\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.4711\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 0.4556\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.4439\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.4480\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 0.4430\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6987\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 0.5284\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 0.4792\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.4594\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.4472\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.4380\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.4427\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.4388\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6504\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.4788\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.4401\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.4267\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.4260\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.4200\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.4144\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.4136\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6397\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.4834\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.4528\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.4476\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.4445\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.4355\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.4354\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.4320\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6572\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.4747\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.4382\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.4404\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.4371\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.4331\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.4283\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.4320\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.6371\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.4550\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.4408\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 0.4373\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.4310\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 0.4243\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.4186\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.4138\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6442 \n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.4811\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.4542\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.4424\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.4384\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.4281\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.4239\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.4112\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6598\n",
      "Epoch 2/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.4838\n",
      "Epoch 3/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.4473\n",
      "Epoch 4/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.4414\n",
      "Epoch 5/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.4404\n",
      "Epoch 6/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.4355\n",
      "Epoch 7/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.4333\n",
      "Epoch 8/8\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.4237\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Epoch 1/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 781us/step - loss: 0.4799\n",
      "Epoch 2/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - loss: 0.4303\n",
      "Epoch 3/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - loss: 0.4240\n",
      "Epoch 4/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 781us/step - loss: 0.4190\n",
      "Epoch 5/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 797us/step - loss: 0.4133\n",
      "Epoch 6/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 809us/step - loss: 0.4108\n",
      "Epoch 7/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 783us/step - loss: 0.4029\n",
      "Epoch 8/8\n",
      "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 795us/step - loss: 0.3973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import Sequential\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(140421)\n",
    "keras.utils.set_random_seed(140421)\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    activation=\"relu\",\n",
    "    layer_config=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the model\n",
    "    \"\"\"\n",
    "    if not layer_config:\n",
    "        layer_config = [(32, 0.1), (64, 0.2), (32, 0.2)]\n",
    "    model = Sequential()\n",
    "    model.add(Input((10,)))\n",
    "    for layer in layer_config:\n",
    "        model.add(Dense(layer[0], activation=activation))\n",
    "        if len(layer) > 0 and layer[1] > 0.0:\n",
    "            model.add(Dropout(layer[1]))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# We can try to stop overfitting and reduce learning rate if we plateu\n",
    "fit_params = {\n",
    "    \"model__callbacks\": [\n",
    "        keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Instantiate the wrapper to be able to use GridSearchCV\n",
    "sk_model = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    # We are using a regularized adam optimizer\n",
    "    optimizer=keras.optimizers.AdamW(),\n",
    ")\n",
    "\n",
    "# We create a pipeline to scale the training data\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", sk_model)\n",
    "])\n",
    "\n",
    "print(sk_model.get_params().keys())\n",
    "# We now define the search parameters\n",
    "activation = [\"relu\", \"tanh\"]\n",
    "layer_configs = [\n",
    "    [(6, 0), (12, 0.1)],\n",
    "    [(6, 0), (12, 0.1), (6, 0.1)],\n",
    "]\n",
    "epochs = 8\n",
    "batches = [1, 16, 32]\n",
    "param_grid = dict(\n",
    "    model__model__activation=activation,\n",
    "    model__model__layer_config=layer_configs,\n",
    "    model__batch_size=batches,\n",
    "    model__epochs=[epochs],\n",
    ")\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring={\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"f1\": \"f1\",\n",
    "        \"recall\": \"recall\",\n",
    "        \"precision\": make_scorer(precision_score, zero_division=0.0),\n",
    "    },\n",
    "    refit=\"f1\",\n",
    ")\n",
    "grid_result = grid.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__batch_size</th>\n",
       "      <th>param_model__model__activation</th>\n",
       "      <th>param_model__model__layer_config</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.720154</td>\n",
       "      <td>0.074360</td>\n",
       "      <td>2.004629</td>\n",
       "      <td>0.155651</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 1, 'model__model__activa...</td>\n",
       "      <td>0.798275</td>\n",
       "      <td>0.817398</td>\n",
       "      <td>0.810953</td>\n",
       "      <td>0.808875</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>4</td>\n",
       "      <td>0.112211</td>\n",
       "      <td>0.358366</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.207364</td>\n",
       "      <td>0.107974</td>\n",
       "      <td>6</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250460</td>\n",
       "      <td>0.082873</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.084215</td>\n",
       "      <td>6</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.686790</td>\n",
       "      <td>0.142206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.901872</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>1.828469</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 1, 'model__model__activa...</td>\n",
       "      <td>0.795651</td>\n",
       "      <td>0.816273</td>\n",
       "      <td>0.796324</td>\n",
       "      <td>0.802749</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.332425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112027</td>\n",
       "      <td>0.155852</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.224678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075505</td>\n",
       "      <td>0.105483</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.638743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324026</td>\n",
       "      <td>0.260849</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.764802</td>\n",
       "      <td>0.018788</td>\n",
       "      <td>1.826072</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 1, 'model__model__activa...</td>\n",
       "      <td>0.817023</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.804576</td>\n",
       "      <td>0.814249</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410628</td>\n",
       "      <td>0.364847</td>\n",
       "      <td>0.347935</td>\n",
       "      <td>0.374470</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.252302</td>\n",
       "      <td>0.255985</td>\n",
       "      <td>0.273596</td>\n",
       "      <td>0.027551</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598592</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.600071</td>\n",
       "      <td>0.047240</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.975638</td>\n",
       "      <td>0.124864</td>\n",
       "      <td>1.826307</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 1, 'model__model__activa...</td>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.799775</td>\n",
       "      <td>0.816579</td>\n",
       "      <td>0.812625</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>2</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.284182</td>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.354467</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>2</td>\n",
       "      <td>0.290441</td>\n",
       "      <td>0.195212</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.253965</td>\n",
       "      <td>0.041947</td>\n",
       "      <td>2</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.589673</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.223152</td>\n",
       "      <td>0.039092</td>\n",
       "      <td>0.350117</td>\n",
       "      <td>0.024941</td>\n",
       "      <td>16</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 16, 'model__model__activ...</td>\n",
       "      <td>0.796025</td>\n",
       "      <td>0.799775</td>\n",
       "      <td>0.796324</td>\n",
       "      <td>0.797375</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022145</td>\n",
       "      <td>0.031317</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>0.308851</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.619508</td>\n",
       "      <td>0.015236</td>\n",
       "      <td>0.336193</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>16</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 16, 'model__model__activ...</td>\n",
       "      <td>0.796025</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.796324</td>\n",
       "      <td>0.796250</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.250076</td>\n",
       "      <td>0.063356</td>\n",
       "      <td>0.359236</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>16</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 16, 'model__model__activ...</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.805399</td>\n",
       "      <td>0.793698</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>6</td>\n",
       "      <td>0.292943</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>0.185092</td>\n",
       "      <td>0.103681</td>\n",
       "      <td>7</td>\n",
       "      <td>0.202206</td>\n",
       "      <td>0.132597</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.119581</td>\n",
       "      <td>0.073356</td>\n",
       "      <td>7</td>\n",
       "      <td>0.531401</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.508447</td>\n",
       "      <td>0.085675</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.686776</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>0.355639</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>16</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 16, 'model__model__activ...</td>\n",
       "      <td>0.810649</td>\n",
       "      <td>0.804649</td>\n",
       "      <td>0.813578</td>\n",
       "      <td>0.809625</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>3</td>\n",
       "      <td>0.194577</td>\n",
       "      <td>0.192248</td>\n",
       "      <td>0.314483</td>\n",
       "      <td>0.233769</td>\n",
       "      <td>0.057081</td>\n",
       "      <td>4</td>\n",
       "      <td>0.112132</td>\n",
       "      <td>0.114180</td>\n",
       "      <td>0.209945</td>\n",
       "      <td>0.145419</td>\n",
       "      <td>0.045634</td>\n",
       "      <td>5</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.656386</td>\n",
       "      <td>0.056059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.763111</td>\n",
       "      <td>0.679456</td>\n",
       "      <td>0.270154</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 32, 'model__model__activ...</td>\n",
       "      <td>0.786277</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.722056</td>\n",
       "      <td>0.768244</td>\n",
       "      <td>0.032921</td>\n",
       "      <td>12</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153143</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.070607</td>\n",
       "      <td>9</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123389</td>\n",
       "      <td>0.042355</td>\n",
       "      <td>0.057319</td>\n",
       "      <td>9</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201807</td>\n",
       "      <td>0.089491</td>\n",
       "      <td>0.083953</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.558790</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.284905</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 32, 'model__model__activ...</td>\n",
       "      <td>0.796025</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.796324</td>\n",
       "      <td>0.796250</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.308320</td>\n",
       "      <td>0.037136</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (6, 0.0)]</td>\n",
       "      <td>{'model__batch_size': 32, 'model__model__activ...</td>\n",
       "      <td>0.764904</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.767817</td>\n",
       "      <td>0.773124</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>11</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.278834</td>\n",
       "      <td>0.215463</td>\n",
       "      <td>0.250762</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200368</td>\n",
       "      <td>0.202578</td>\n",
       "      <td>0.156538</td>\n",
       "      <td>0.186495</td>\n",
       "      <td>0.021202</td>\n",
       "      <td>3</td>\n",
       "      <td>0.362126</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.345528</td>\n",
       "      <td>0.384936</td>\n",
       "      <td>0.044514</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.597955</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.277837</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[(6, 0), (12, 0.1)]</td>\n",
       "      <td>{'model__batch_size': 32, 'model__model__activ...</td>\n",
       "      <td>0.767904</td>\n",
       "      <td>0.797150</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.777499</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>10</td>\n",
       "      <td>0.376636</td>\n",
       "      <td>0.065630</td>\n",
       "      <td>0.198966</td>\n",
       "      <td>0.213744</td>\n",
       "      <td>0.127397</td>\n",
       "      <td>5</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.141805</td>\n",
       "      <td>0.173515</td>\n",
       "      <td>0.128029</td>\n",
       "      <td>4</td>\n",
       "      <td>0.416481</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.425864</td>\n",
       "      <td>0.079658</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        4.720154      0.074360         2.004629        0.155651   \n",
       "1        4.901872      0.060802         1.828469        0.019548   \n",
       "2        4.764802      0.018788         1.826072        0.015068   \n",
       "3        4.975638      0.124864         1.826307        0.026365   \n",
       "4        1.223152      0.039092         0.350117        0.024941   \n",
       "5        1.619508      0.015236         0.336193        0.019671   \n",
       "6        1.250076      0.063356         0.359236        0.011378   \n",
       "7        1.686776      0.016382         0.355639        0.010256   \n",
       "8        1.763111      0.679456         0.270154        0.016512   \n",
       "9        1.558790      0.026766         0.284905        0.012145   \n",
       "10       1.308320      0.037136         0.281900        0.010966   \n",
       "11       1.597955      0.005718         0.277837        0.009805   \n",
       "\n",
       "    param_model__batch_size param_model__model__activation  \\\n",
       "0                         1                           relu   \n",
       "1                         1                           relu   \n",
       "2                         1                           tanh   \n",
       "3                         1                           tanh   \n",
       "4                        16                           relu   \n",
       "5                        16                           relu   \n",
       "6                        16                           tanh   \n",
       "7                        16                           tanh   \n",
       "8                        32                           relu   \n",
       "9                        32                           relu   \n",
       "10                       32                           tanh   \n",
       "11                       32                           tanh   \n",
       "\n",
       "   param_model__model__layer_config  \\\n",
       "0                [(6, 0), (6, 0.0)]   \n",
       "1               [(6, 0), (12, 0.1)]   \n",
       "2                [(6, 0), (6, 0.0)]   \n",
       "3               [(6, 0), (12, 0.1)]   \n",
       "4                [(6, 0), (6, 0.0)]   \n",
       "5               [(6, 0), (12, 0.1)]   \n",
       "6                [(6, 0), (6, 0.0)]   \n",
       "7               [(6, 0), (12, 0.1)]   \n",
       "8                [(6, 0), (6, 0.0)]   \n",
       "9               [(6, 0), (12, 0.1)]   \n",
       "10               [(6, 0), (6, 0.0)]   \n",
       "11              [(6, 0), (12, 0.1)]   \n",
       "\n",
       "                                               params  split0_test_accuracy  \\\n",
       "0   {'model__batch_size': 1, 'model__model__activa...              0.798275   \n",
       "1   {'model__batch_size': 1, 'model__model__activa...              0.795651   \n",
       "2   {'model__batch_size': 1, 'model__model__activa...              0.817023   \n",
       "3   {'model__batch_size': 1, 'model__model__activa...              0.821522   \n",
       "4   {'model__batch_size': 16, 'model__model__activ...              0.796025   \n",
       "5   {'model__batch_size': 16, 'model__model__activ...              0.796025   \n",
       "6   {'model__batch_size': 16, 'model__model__activ...              0.800900   \n",
       "7   {'model__batch_size': 16, 'model__model__activ...              0.810649   \n",
       "8   {'model__batch_size': 32, 'model__model__activ...              0.786277   \n",
       "9   {'model__batch_size': 32, 'model__model__activ...              0.796025   \n",
       "10  {'model__batch_size': 32, 'model__model__activ...              0.764904   \n",
       "11  {'model__batch_size': 32, 'model__model__activ...              0.767904   \n",
       "\n",
       "    split1_test_accuracy  split2_test_accuracy  mean_test_accuracy  \\\n",
       "0               0.817398              0.810953            0.808875   \n",
       "1               0.816273              0.796324            0.802749   \n",
       "2               0.821147              0.804576            0.814249   \n",
       "3               0.799775              0.816579            0.812625   \n",
       "4               0.799775              0.796324            0.797375   \n",
       "5               0.796400              0.796324            0.796250   \n",
       "6               0.805399              0.793698            0.799999   \n",
       "7               0.804649              0.813578            0.809625   \n",
       "8               0.796400              0.722056            0.768244   \n",
       "9               0.796400              0.796324            0.796250   \n",
       "10              0.786652              0.767817            0.773124   \n",
       "11              0.797150              0.767442            0.777499   \n",
       "\n",
       "    std_test_accuracy  rank_test_accuracy  split0_test_f1  split1_test_f1  \\\n",
       "0            0.007944                   4        0.112211        0.358366   \n",
       "1            0.009567                   5        0.003656        0.332425   \n",
       "2            0.007044                   1        0.410628        0.364847   \n",
       "3            0.009308                   2        0.398990        0.284182   \n",
       "4            0.001702                   7        0.000000        0.066434   \n",
       "5            0.000162                   8        0.000000        0.000000   \n",
       "6            0.004819                   6        0.292943        0.217195   \n",
       "7            0.003716                   3        0.194577        0.192248   \n",
       "8            0.032921                  12        0.006969        0.000000   \n",
       "9            0.000162                   8        0.000000        0.000000   \n",
       "10           0.009639                  11        0.257988        0.278834   \n",
       "11           0.013897                  10        0.376636        0.065630   \n",
       "\n",
       "    split2_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \\\n",
       "0         0.151515      0.207364     0.107974             6   \n",
       "1         0.000000      0.112027     0.155852             8   \n",
       "2         0.347935      0.374470     0.026483             1   \n",
       "3         0.380228      0.354467     0.050285             2   \n",
       "4         0.000000      0.022145     0.031317            10   \n",
       "5         0.000000      0.000000     0.000000            11   \n",
       "6         0.045139      0.185092     0.103681             7   \n",
       "7         0.314483      0.233769     0.057081             4   \n",
       "8         0.153143      0.053370     0.070607             9   \n",
       "9         0.000000      0.000000     0.000000            11   \n",
       "10        0.215463      0.250762     0.026371             3   \n",
       "11        0.198966      0.213744     0.127397             5   \n",
       "\n",
       "    split0_test_recall  split1_test_recall  split2_test_recall  \\\n",
       "0             0.062500            0.250460            0.082873   \n",
       "1             0.001838            0.224678            0.000000   \n",
       "2             0.312500            0.252302            0.255985   \n",
       "3             0.290441            0.195212            0.276243   \n",
       "4             0.000000            0.034991            0.000000   \n",
       "5             0.000000            0.000000            0.000000   \n",
       "6             0.202206            0.132597            0.023941   \n",
       "7             0.112132            0.114180            0.209945   \n",
       "8             0.003676            0.000000            0.123389   \n",
       "9             0.000000            0.000000            0.000000   \n",
       "10            0.200368            0.202578            0.156538   \n",
       "11            0.343750            0.034991            0.141805   \n",
       "\n",
       "    mean_test_recall  std_test_recall  rank_test_recall  \\\n",
       "0           0.131944         0.084215                 6   \n",
       "1           0.075505         0.105483                 8   \n",
       "2           0.273596         0.027551                 1   \n",
       "3           0.253965         0.041947                 2   \n",
       "4           0.011664         0.016495                10   \n",
       "5           0.000000         0.000000                11   \n",
       "6           0.119581         0.073356                 7   \n",
       "7           0.145419         0.045634                 5   \n",
       "8           0.042355         0.057319                 9   \n",
       "9           0.000000         0.000000                11   \n",
       "10          0.186495         0.021202                 3   \n",
       "11          0.173515         0.128029                 4   \n",
       "\n",
       "    split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0                0.548387               0.629630               0.882353   \n",
       "1                0.333333               0.638743               0.000000   \n",
       "2                0.598592               0.658654               0.542969   \n",
       "3                0.637097               0.522167               0.609756   \n",
       "4                0.000000               0.655172               0.000000   \n",
       "5                0.000000               0.000000               0.000000   \n",
       "6                0.531401               0.600000               0.393939   \n",
       "7                0.734940               0.607843               0.626374   \n",
       "8                0.066667               0.000000               0.201807   \n",
       "9                0.000000               0.000000               0.000000   \n",
       "10               0.362126               0.447154               0.345528   \n",
       "11               0.416481               0.527778               0.333333   \n",
       "\n",
       "    mean_test_precision  std_test_precision  rank_test_precision  \n",
       "0              0.686790            0.142206                    1  \n",
       "1              0.324026            0.260849                    8  \n",
       "2              0.600071            0.047240                    3  \n",
       "3              0.589673            0.049022                    4  \n",
       "4              0.218391            0.308851                    9  \n",
       "5              0.000000            0.000000                   11  \n",
       "6              0.508447            0.085675                    5  \n",
       "7              0.656386            0.056059                    2  \n",
       "8              0.089491            0.083953                   10  \n",
       "9              0.000000            0.000000                   11  \n",
       "10             0.384936            0.044514                    7  \n",
       "11             0.425864            0.079658                    6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with pd.option_context('display.max_columns', None, 'display.max_rows', None):\n",
    "    display(pd.DataFrame(grid_result.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__batch_size': 1, 'model__model__activation': 'tanh', 'model__model__layer_config': [(6, 0), (6, 0.0)]} 0.37446993627028774\n"
     ]
    }
   ],
   "source": [
    "print(grid_result.best_params_, grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90      1593\n",
      "           1       0.70      0.25      0.37       407\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.77      0.61      0.64      2000\n",
      "weighted avg       0.81      0.83      0.79      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred = grid_result.best_estimator_.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=grid_result.best_estimator_.classes_)\n",
    "disp.plot()\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que incrementar el batch_size ha sido prejudicial para el aprendizaje del modelo, todos los modelos con batch_size mayor que 1 tienen peor rendimiento que aquellos que van de 1 en 1. La función de activación tanh parece ir mejor con un batch_size más grande. La configuración más sencilla (6, 6) es mejor que (6, 12).\n",
    "\n",
    "Con el mejor modelo tenemos un buen F1 para el caso negativo. No obstante, el resultado para la clase positiva deja que desear. Sería interesante experimentar de forma más exhaustiva con fin de optimizar el caso positivo. Quizás, técnicas de sub / sobre muestreo puedan ayudar a confrontar el desequilibrio y que el modelo sea capaz de modelar mejor las clases positivas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
